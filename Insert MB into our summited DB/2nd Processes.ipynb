{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Path Dirction :: C:\\AhmedElsayed-Reliable Work Space\\Insert MB Data in our database\n",
      "----------------\n",
      "\n",
      "The Content Path\n",
      "--------------------\n",
      "11.2.0.4.0\n",
      "['EGX100LASTEWIMB', 35.0, 67.0, 50.0, 52.0, -15.0, datetime.datetime(2025, 3, 19, 0, 0, tzinfo=<UTC>), 1, 1] That Record didn't Insert To our dataBase\n",
      "ORA-00001: unique constraint (STOCK.FILL_OHLCV_U01) violated\n",
      "11.2.0.4.0\n",
      "['EGX30LASTEWIMB', 8.0, 24.0, 19.0, 13.0, -11.0, datetime.datetime(2025, 3, 19, 0, 0, tzinfo=<UTC>), 1, 1] That Record didn't Insert To our dataBase\n",
      "ORA-00001: unique constraint (STOCK.FILL_OHLCV_U01) violated\n",
      "11.2.0.4.0\n",
      "['EGX50LASTEWIMB', 15.0, 35.0, 27.0, 23.0, -12.0, datetime.datetime(2025, 3, 19, 0, 0, tzinfo=<UTC>), 1, 1] That Record didn't Insert To our dataBase\n",
      "ORA-00001: unique constraint (STOCK.FILL_OHLCV_U01) violated\n",
      "11.2.0.4.0\n",
      "['EGX70LASTEWIMB', 27.0, 43.0, 31.0, 39.0, -4.0, datetime.datetime(2025, 3, 19, 0, 0, tzinfo=<UTC>), 1, 1] That Record didn't Insert To our dataBase\n",
      "ORA-00001: unique constraint (STOCK.FILL_OHLCV_U01) violated\n",
      "Done\n",
      " Saved MB data Values done  Done\n",
      "=================================\n",
      "=================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "from datetime import datetime as dt\n",
    "from datetime import date \n",
    "import csv , json\n",
    "import time\n",
    "import os \n",
    "#-------------------------------------------------------------------------\n",
    "#-------------------------------------------------------------------------\n",
    "# Access on Orcale DataBase :\n",
    "def dbConnect():\n",
    "    '''\n",
    "    creates a standalone connection with the database\n",
    "    parameters:\n",
    "        none\n",
    "        \n",
    "    return: \n",
    "       con: cx_oracle connection\n",
    "    '''\n",
    "    \n",
    "    con = cx_Oracle.connect('STOCK/P3rXdM5HbSgQRmCS@10.1.20.41:1521/STOCK')\n",
    "    print (con.version)\n",
    "    return con\n",
    "#-------------------------------------------------------------------------\n",
    "#-------------------------------------------------------------------------\n",
    "Excelfiles_path      = \"Y:\\Asset Management-Data science\\PharosQuantitativeTeam\\DataSets-For-Import Wizard\\DataFeed Data\\myexcelmarketbreadthfiles\"\n",
    "Csvfile_path         = \"Y:\\Asset Management-Data science\\PharosQuantitativeTeam\\DataSets-For-Import Wizard\\DataFeed Data\\myexcelmarketbreadthfiles\\MB-Data-Processing\\EGXs Long && Short seperated Files\"\n",
    "#FinalCsvfile_path    = \"Y:\\Asset Management-Data science\\PharosQuantitativeTeam\\DataSets-For-Import Wizard\\DataFeed Data\\myexcelmarketbreadthfiles\\MB-Data-Processing\"\n",
    "FinalCsvfile_path    = \"Y:\\Asset Management-Data science\\PharosQuantitativeTeam\\DataSets-For-Import Wizard\\DataFeed Data\\myexcelmarketbreadthfiles\\MB-Data-Processing\\Final EGXs MB Data\"\n",
    "\n",
    "#-------------------------------------------------------------------------\n",
    "#-------------------------------------------------------------------------\n",
    "df_Long_EGX30   = pd.read_excel(f\"{Excelfiles_path}/MarketBreadth-LongCountOnly-EGX30.xlsx\")\n",
    "df_Short_EGX30  = pd.read_excel(f\"{Excelfiles_path}/MarketBreadth-ShortCountOnly-EGX30.xlsx\")\n",
    "df_Long_EGX50   = pd.read_excel(f\"{Excelfiles_path}/MarketBreadth-LongCountOnly-EGX50.xlsx\")\n",
    "df_Short_EGX50  = pd.read_excel(f\"{Excelfiles_path}/MarketBreadth-ShortCountOnly-EGX50.xlsx\")\n",
    "df_Long_EGX70   = pd.read_excel(f\"{Excelfiles_path}/MarketBreadth-LongCountOnly-EGX70.xlsx\")\n",
    "df_Short_EGX70  = pd.read_excel(f\"{Excelfiles_path}/MarketBreadth-ShortCountOnly-EGX70.xlsx\")\n",
    "df_Long_EGX100  = pd.read_excel(f\"{Excelfiles_path}/MarketBreadth-LongCountOnly-EGX100.xlsx\")\n",
    "df_Short_EGX100 = pd.read_excel(f\"{Excelfiles_path}/MarketBreadth-ShortCountOnly-EGX100.xlsx\")\n",
    "#-------------------------------------------------------------------------\n",
    "#-------------------------------------------------------------------------\n",
    "todaydater      = date.today().strftime('%Y-%m-%d')\n",
    "df_Long_EGX30   = df_Long_EGX30[df_Long_EGX30[\"Date\"]     ==todaydater]\n",
    "df_Short_EGX30  = df_Short_EGX30[df_Short_EGX30[\"Date\"]   ==todaydater]\n",
    "df_Long_EGX50   = df_Long_EGX50[df_Long_EGX50[\"Date\"]     ==todaydater]\n",
    "df_Short_EGX50  = df_Short_EGX50[df_Short_EGX50[\"Date\"]   ==todaydater]\n",
    "df_Long_EGX70   = df_Long_EGX70[df_Long_EGX70[\"Date\"]     ==todaydater]\n",
    "df_Short_EGX70  = df_Short_EGX70[df_Short_EGX70[\"Date\"]   ==todaydater]\n",
    "df_Long_EGX100  = df_Long_EGX100[df_Long_EGX100[\"Date\"]   ==todaydater]\n",
    "df_Short_EGX100 = df_Short_EGX100[df_Short_EGX100[\"Date\"] ==todaydater]\n",
    "#-------------------------------------------------------------------------\n",
    "#-------------------------------------------------------------------------\n",
    "EGX30_merged_df = pd.merge(df_Long_EGX30, df_Short_EGX30, on='Date', how='inner')\n",
    "EGX30_merged_df[\"Long - Short Cout\"] = EGX30_merged_df['Long Count']  - EGX30_merged_df['Short Count'] \n",
    "EGX30_merged_df[\"TICKER\"] = \"EGX30LASTEWIMB\"\n",
    "EGX30_merged_df = EGX30_merged_df[[\"Date\" ,\"TICKER\",'Long Count', 'Exit Long Count', 'Short Count',\n",
    "                                  'Exit Short Count',\"Long - Short Cout\"]]\n",
    "#-------------------------------------------------------------------------\n",
    "#-------------------------------------------------------------------------\n",
    "EGX50_merged_df = pd.merge(df_Long_EGX50, df_Short_EGX50, on='Date', how='inner')\n",
    "EGX50_merged_df[\"TICKER\"] = \"EGX50LASTEWIMB\"\n",
    "EGX50_merged_df[\"Long - Short Cout\"] = EGX50_merged_df['Long Count']  - EGX50_merged_df['Short Count'] \n",
    "EGX50_merged_df = EGX50_merged_df[[\"Date\" ,\"TICKER\",'Long Count', 'Exit Long Count', 'Short Count',\n",
    "                                  'Exit Short Count',\"Long - Short Cout\"]]\n",
    "#-------------------------------------------------------------------------\n",
    "#-------------------------------------------------------------------------\n",
    "EGX70_merged_df = pd.merge(df_Long_EGX70, df_Short_EGX70, on='Date', how='inner')\n",
    "EGX70_merged_df[\"TICKER\"] = \"EGX70LASTEWIMB\"\n",
    "EGX70_merged_df[\"Long - Short Cout\"] = EGX70_merged_df['Long Count']  - EGX70_merged_df['Short Count'] \n",
    "EGX70_merged_df = EGX70_merged_df[[\"Date\" ,\"TICKER\",'Long Count', 'Exit Long Count', 'Short Count',\n",
    "                                  'Exit Short Count',\"Long - Short Cout\"]]\n",
    "#-------------------------------------------------------------------------\n",
    "#-------------------------------------------------------------------------\n",
    "EGX100_merged_df = pd.merge(df_Long_EGX100, df_Short_EGX100, on='Date', how='inner')\n",
    "EGX100_merged_df[\"TICKER\"] = \"EGX100LASTEWIMB\"\n",
    "EGX100_merged_df[\"Long - Short Cout\"] = EGX100_merged_df['Long Count']  - EGX100_merged_df['Short Count'] \n",
    "EGX100_merged_df = EGX100_merged_df[[\"Date\" ,\"TICKER\",'Long Count', 'Exit Long Count', 'Short Count',\n",
    "                                  'Exit Short Count',\"Long - Short Cout\"]]\n",
    "#-------------------------------------------------------------------------\n",
    "#-------------------------------------------------------------------------\n",
    "#-------------------------------------------------------------------------\n",
    "#-------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "path = \"Y:/Asset Management-Data science/PharosQuantitativeTeam/DataSets-For-Import Wizard/DataFeed Data/myexcelmarketbreadthfiles/MB-Data-Processing\"\n",
    "EGXsMBlists     = [\"(FinalEGX30MBLong&Short).csv\"    , \"(FinalEGX50MBLong&Short).csv\",\n",
    "                  \"(FinalEGX70MBLong&Short).csv\", \"(FinalEGX100MBLong&Short).csv\"] \n",
    "EGXsMBdflists   = [EGX30_merged_df , EGX50_merged_df, EGX70_merged_df, EGX100_merged_df]\n",
    "\n",
    "\n",
    "for (EGXsMB,EGXsMBdf) in  zip(EGXsMBlists, EGXsMBdflists): \n",
    "    for index, row in EGXsMBdf.iterrows():\n",
    "        \n",
    "        EGXdate    = row[\"Date\"]\n",
    "        EGXticker  = row[\"TICKER\"  ]\n",
    "        EGX_L_C    = row[\"Long Count\"]\n",
    "        EGX_E_L_C  = row[\"Exit Long Count\"]\n",
    "        EGX_S_C    = row[\"Short Count\"]\n",
    "        EGX_E_S_C  = row[\"Exit Short Count\"]\n",
    "        EGX_LC_SC  = row[\"Long - Short Cout\"]\n",
    "\n",
    "        with open(f\"{path}/{EGXsMB}\" , \"a\" , encoding = \"utf-8\",newline='') as ff :\n",
    "            writer = csv.DictWriter(ff, fieldnames=[\"Date\",\"Ticker\"    , \"Long Count\" ,\n",
    "                                                    \"Exit Long Count\"  , \"Short Count\",\n",
    "                                                    \"Exit Short Count\" , \"Long - Short Cout\"])\n",
    "            \n",
    "            writer.writerow({\"Date\"   : EGXdate ,\"Ticker\" : EGXticker,\"Long Count\" : EGX_L_C,\n",
    "                            \"Exit Long Count\"   : EGX_E_L_C ,\"Short Count\" : EGX_S_C,\n",
    "                            \"Exit Short Count\"  : EGX_E_S_C,\"Long - Short Cout\" : EGX_LC_SC})\n",
    "#----------------------------------#----------------------------------\n",
    "#----------------------------------#----------------------------------\n",
    "path = os.path.abspath('') \n",
    "#path1 =\"E:/AhmadElsayed_WorkSpace/All Data Preprocessing/TickerChart_Data/Tickers_(NETFLOW)Stand alone/New folder\"\n",
    "print(f\"The Path Dirction :: {path}\\n----------------\\n\\nThe Content Path\\n--------------------\")\n",
    "# Extract all files which ended with csv.\n",
    "files = os.listdir(path)\n",
    "filelis = []\n",
    "for file in files :\n",
    "    if file.endswith('.csv'):\n",
    "        filelis.append(file)\n",
    "        tbIns= pd.read_csv(file)\n",
    "        #tbIns['TICKER'] =tbIns['TICKER'].apply(lambda x: x.split(\" \")[0]) +\"FLOW\"\n",
    "        tbIns['Ticker'] =tbIns['Ticker'].apply(lambda x: x.replace(\" \",\"\"))\n",
    "        tbIns['Date'] = pd.to_datetime(tbIns['Date'],  errors='coerce', utc=True)\n",
    "        tbIns.set_index(\"Date\" , inplace=True)\n",
    "        tbIns.sort_index(ascending = True, inplace=True)\n",
    "        #-----------------------------------------------------------\n",
    "        tbIns =tbIns.tail(1)\n",
    "        con=dbConnect()\n",
    "        cur = con.cursor()\n",
    "        lines=[]\n",
    "        for index,row in tbIns.iterrows():\n",
    "            try:\n",
    "                line=[0,1,2,3,4,5,6,7,8]\n",
    "                line[0]=row[\"Ticker\"]\n",
    "                #line[0]=\"AARRTRY\"\n",
    "                line[1]=row['Long Count']\n",
    "                line[2]=row['Exit Long Count']\n",
    "                line[3]=row['Short Count']\n",
    "                line[4]=row['Exit Short Count']\n",
    "                line[5]=row['Long - Short Cout']\n",
    "                line[6]=index.to_pydatetime()\n",
    "                line[7]=1\n",
    "                line[8]=1\n",
    "                #print(index.to_pydatetime())\n",
    "                lines.append(line)\n",
    "                #print(lines)\n",
    "                cur.execute(\"insert into FILL_OHLCV(TICKER,OPEN,HIGH,LOW,CLOSE,VOLUME,BARTIMESTAMP,ASSET,VWAP) values (:TICKER, :OPEN,:HIGH,:LOW,:CLOSE,:VOLUME,:BARTIMESTAMP,:1,:2)\",line)\n",
    "                con.commit()\n",
    "                print(f\"{line} That Record Inserted To our dataBase\")\n",
    "                #print(line)\n",
    "            except Exception as e:\n",
    "                #cur.execute(\"insert into FILL_OHLCV(TICKER,OPEN,HIGH,LOW,CLOSE,VOLUME,BARTIMESTAMP,ASSET,VWAP) values (:TICKER, :OPEN,:HIGH,:LOW,:CLOSE,:VOLUME,:BARTIMESTAMP,:1,:2)\",line)\n",
    "                #con.commit()\n",
    "                #print(\"4444\")\n",
    "                print(f\"{line} That Record didn't Insert To our dataBase\")\n",
    "                print(str(e))\n",
    "                #print(line)\n",
    "                \n",
    "                \n",
    "print(\"Done\")\n",
    "print(f\" Saved MB data Values done  Done\")\n",
    "print(\"=================================\")\n",
    "print(\"=================================\")\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
